<!DOCTYPE html>
<!-- <html style="background-color:#666666"> -->
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <title>BIR - Big Data Retriever</title>
    <link rel="stylesheet" href="css/scholarly.css"> <!-- css/scholarly.min.css -->
    <script src="js/scholarly.min.js"></script>
  </head>
  <body prefix="schema: http://schema.org">
    <header>
      <div class="banner">
        <img src="LOGO.png" width="227" height="50" alt="BIR logo">
        <div class="status" style="background-color:#db7b27">WADe Project</div>
      </div>
      <h2>BIR - Big Data Retriever Scholarly HTML Technical Report</h2>
    </header>

     <div role="contentinfo">
      <dl>
        <dt>Authors</dt>
        <dd>
          <p>Andrei Palihovici, Student, M.Sc. in Computational Linguistics</p>
          <p>Ileana Popa, Student, M.Sc. in Software Engineering</p>
        </dd>
      </dl>
    </div>
    <section typeof="sa:Abstract" id="abstract" role="doc-abstract">
      <h2>Abstract</h2>
      <p>
        BIR- Big Data Retriever is a Web System that is able to query, compare, visualize, share,
        summarize, large sets of data/knowledge and additional resources provided by Wikidata.
  The platform will give the user access to everything that he finds of interest that is available
        in the Wikidata collaboratively edited knowledge base, items and topics ranging from
        1988 Summer Olympics, to love or Elvis Presley. Every item in this valuable collection of data is
  uniquely identified by an unique number, making it possible for the concept to be translated without
  favoring any language. The tool can for this reason easily recommend related information and similar
  resources available in other languages. In the background, data will be processed in a distributed,
  fast and reliable manner, using Hadoop MapReduce.

      </p>
    </section>


    <section id="introduction" role="doc-introduction">
      <!-- review? -->
      <h2>Architecture</h2>
      <p>The user connects to the Web Interface of the BIR project, where the topics can be chosen and a query is
      generated, not unlike the tool available <a href="https://tools.wmflabs.org/hay/vizquery/">here</a>. After the user
      describes the data they are interested in, the input is transformed in a SPARQL query. </p>
      <p>The suggestions module prepares all the queries necessary that include new related data that might be of interest. Wikidata databases 
      are queried through the SPARQL endpoint provided and the results are generated in JSON format. All the data is then uploaded to Hadoop
      and using Hive and Impala, the final results that will be displayed for the user are sent to the Translations Module that is based on the
      item codes can generate data in multiple languages without having a bias for a particular one, the focus being on the knowledge itself, and
      not constrained by a specific translation of the concepts.</p>
      <img src="arch_diagram.png" style="width: 800px;" alt="Architecture Diagram">
    </section>


    <section id="structure">
      <!-- review? -->
      <h2>Internal data structures/models</h2>
      <section id="Root">
        <!-- review? -->
        <h3>Data format choice JSON/RDF</h3>
        <p>
            RDF stands for Resource Description Framework and is a standard for data interchange, and a mature,
            thoroughly-tested and robust technology for modelling data. Representing data in RDF allows
            information to be identified, disambiguated and inter-connected by software agents and various
            systems in order to be read, analyzed or acted upon.
        </p>
        <p>
            RDF serves as an uniform structure to express any kind of information using simple statements, not depending
            on their source. All data, regardless of their origin, can be converted to RDF data.
        </p>
        <p>
            Wikidata supports RDF representation of data and is able to return answers to queries in this format.
            RDF data dumps are available as well.
        </p>
        <p>
            Currently, the JSON format was considered as the format of choice for the initial test phases 
            when requesting data from the SparQL endpoint rather than RDF, especially for the convenience 
            of writing the data in a key/value dictionary that is the main format used in the MapReduce jobs integrated with the application.
        </p>
        <p>
            In the next stage, we will explore the Map-Reduce based SPARQL processing engine, called SHOE 
            (SPARQL on Hadoop with Optimization Encoding) to handle billions of RDF triples. 
            SHOE consists of three major components: 
            (1) the RDF data loader, 
            (2) the partition generator
            (3) the query processor. 
            While this demonstration mainly focuses on enhancing SPARQL processing in the Hadoop platform.
        </p>

      </section>
      <section id="article">
        <!-- review? -->
        <h3>Data for Suggestions and processing with MapReduce</h3>
        <p>
          Hadoop distributes the data as it is initially stored in the system. Many nodes work in parallel, 
          each on their own part of the overall dataset. While preparing the data, extracting knowledge from it
          and making relevant suggestions based on the user's interests, this Big Data approach can tackle problems
          like <it>Graph creation and analysis</it> and <it>Pattern Recognition</it> that proves to be meaningful for
          the problem at hand. 
        </p>
        <img src="map_reduce.png" width="600px" alt="MAP REDUCE">

        <p>A python module will be responsible for creating suggestions based on the user input. This extended data will be
        aggregated and using Hive and Impala will be processed with Hadoop to prepare the Suggestions Results.</p>
      </section>
    </section>
    </section>

    <section id="semantics">

      <h2>External Data Sources</h2>

      <section id="person-org">
        <h3>Wikidata</h3>
        <p>Belonging to the Wikimedia family of websites, alongside the famous Wikipedia, Wikidata is a place that
        stores structured data in many languages. The basic entity stored is an Item and can be a thing, a place,
        a person, an idea or anything else. Each article from Wikipedia corresponds to a Wikidata Item, but there
        are many Wikidata Items that have no Wikipedia pages associated to them.</p>

        <p>All languages are treated the same way because Items are not identified by specific words, but by unique
        identifiers prefixed with the letter Q, known as "QID". This allows the item to be translated without any
        language bias. Also, some items named in the same way can have different meaning, thus being identified by different codes.
        For example, Elvis Presley (Q303) represents the American singer and actor and Elvis Presley (Q610926) represents the self titled album.</p>

        <p>Similarly to items, Wikidata stores a list of Statements associated to an item. Each statement has a Property and a Value, and the Property
        has a generic identifier that starts with P instead of Q. For example the item Q1490 (Tokyo) has associated the value P17 that represents
        "country" in English, "государство" in Russian and so on. The value P17(contry) for Q1490(Tokyo) is Q17(Japan).</p>

        <p>Not all statement have to point to numeric values, for example one of the statements about Tokyo is the flag Q20900820, which points to an
          image of the <a href="https://commons.wikimedia.org/wiki/File:Flag_of_Tokyo_Prefecture.svg">Flag of Tokyo Prefecture</a>.</p>

        <p>Data provided by Wikidata can transform into knowledge or interesting facts, as Amir E. Aharoni, that works at Wikimedia Foundation
        pointed out in a Quora answer. In this <a href="https://commons.wikimedia.org/wiki/File:Flag_of_Tokyo_Prefecture.svg">article</a> Buzzfeed came
        up with a list of the 30 most famous people to die each year since 1900.</p>
      </section>

      <section id="typing-sections">
        <h3>SparQL Endpoint</h3>
        <p>
        The service provided by <a href="https://query.wikidata.org">Wikidata Query Service</a> is used programatically from python by submitting requests to
        https://query.wikidata.org/sparql, as shown in the code below. The requests module handles HTTP requests, runs a SPARQL query and can return the resulted
        data in XML, CSV, JSON or BINARY RDF.
        </p>
       <p> <pre><code style="font-size: 12.5px;">
          import requests

          url = <b style="color: #6b6b6b; /*light gray*/">'https://query.wikidata.org/sparql'</b>
          <b style="color:#eb8934;/*orange*/">
          query = r"""
          #Illustrates optional fields, instances of subclasses, language fallback on label service, date to
          #year conversion

          #Horses on Wikidata
          SELECT DISTINCT ?horse ?horseLabel ?mother ?father (year(?birthdate) as ?birthyear) (year(?deathdate) \
          as ?deathyear) ?genderLabel
          WHERE
          {
            ?horse wdt:P31/wdt:P279* wd:Q726 .     # Instance et sous-classes de Q726-Cheval

            OPTIONAL{?horse wdt:P25 ?mother .}       # P25  : Mère
            OPTIONAL{?horse wdt:P22 ?father .}       # P22  : Père
            OPTIONAL{?horse wdt:P569 ?birthdate .} # P569 : Date de naissance
            OPTIONAL{?horse wdt:P570 ?deathdate .}     # P570 : Date de décès
            OPTIONAL{?horse wdt:P21 ?gender .}       # P21  : Sexe

            SERVICE wikibase:label { #BabelRainbow
              bd:serviceParam wikibase:language "[AUTO_LANGUAGE],fr,ar,be,bg,bn,ca,cs,da,de,el,en,es,et,fa,fi,
              he,hi,hu,hy,id,it,ja,jv,ko,nb,nl,eo,pa,pl,pt,ro,ru,sh,sk,sr,sv,sw,te,th,tr,uk,yue,vec,vi,zh"
            }
          }
          ORDER BY ?horse
          """
          </b>
          r = requests.get(url, params = {'format': 'json', 'query': query})
          data = r.json()

          print(data)
      </code></pre></p>
      </section>

    </section>

    <section id="scholarly-article">
      <!-- review? -->
      <h2>User Interaction</h2>
      <img src="usecase.PNG" width="100%">

        </img>
      <p>
          The user is first presented with a homescreen that displays all the available options:
      </p>

    <p>
        <i>Create query</i> allows the user to create a new query and select either Wikidata, DBPedia or both as the knowledge base.
        The user also has the choice of using either the built-in GUI for creating queries or using <i>advanced syntax</i> in
        order to write the query manually themselves. The results are nevertheless the same.
    </p>
    <p>
        <i>Recent</i> shows the 10 previous queries inputed by the user, provided cookies are enabled.
    </p>
    <p>
        <i>Examples</i> displays a list of possible queries in both simple and advanced syntax.
    </p>
    <p>
        Postprocessing is also available via additional filtering and grouping methods (such as grouping results by country or time spans etc).
    </p>
    </section>
    <section id="hypermedia">
      <!-- review? -->
      <h2>User Interface</h2>
      <p>
        For basic queries, the user can use the <it>Simple Query Interface</it> to specify rules for
        accessing data: a property and an item that will be mapped to the internal codes used in Wikidata.
      </p>
      <img src="mock1.png" width="600px" alt="Mock Simple Query">
      <p class="issue">
        For more complex queries, the <it>SPARQL Query</it> interface is available, where the entire query 
        can be specified. This USE CASE lets the user have more power over the retreived data, being able
        to pick any fields in any order visible in the item's <a href="https://www.wikidata.org/wiki/Q378619">page</a>.
      </p>
      <img src="mock2.png" width="600px" alt="Mock SPARQL Query">
      
    </section>
    <section id="acks">
      <!-- review? -->
      <h2>Conclusions and Improvement</h2>
      <p>BiR set out to be a great visualization and aggregator for Big Data and is shaping up nicely. Its intended purpose is to make exploratory analysis of resources
      such as those provided by Wikidata or DBPedia much easier, and bring new, intuitive and descriptive visual representations to the table.</p>
      <p>
          Using JavaScript and Python, BiR is very lightweight and completely platform-independent; being available on virtually any OS. Full-fledged support for mobile versions
          is in the works as well.
      </p>

    </section>

    <section id="refs">
      <h2>References</h2>
      <a href="https://www.wikidata.org/wiki/Wikidata:Data_access">https://www.wikidata.org/wiki/Wikidata:Data_access</a>
      <br>
      <a href="https://tools.wmflabs.org/hay/directory/#/search/wikidata">Tools Directory</a>
      <br>
      <a href="https://www.quora.com/What-is-Wikidata">https://www.quora.com/What-is-Wikidata</a>
      <br>
      <a href="https://towardsdatascience.com/where-do-mayors-come-from-querying-wikidata-with-python-and-sparql-91f3c0af22e2">https://towardsdatascience.com/where-do-mayors-come-from-querying-wikidata-with-python-and-sparql-91f3c0af22e2</a>

    </section>

  </body>
</html>
